import wandb

# Initialize W&B
wandb.init(
    project="TITAN_Project",  # Change to your project name
    name="Titan_Run1",        # Change to a unique run name
    config={
        "model": "TITAN",
        "dataset": "TCGA",
        "epochs": 10,  # Update if training
        "batch_size": 32,
    }
)



import numpy
import torch
import transformers
import h5py



import os
os.environ["PATH"] = "/usr/local/miniconda/bin:" + os.environ["PATH"]


import torchvision
print(torchvision.__version__)

#!pip install huggingface_hub

from huggingface_hub import login

login()

"""**TITAN (Transformer-based pathology Image and Text Alignment Network)**

The pretraining strategy consists of three distinct stages to ensure that the resulting slide-level representations capture histomorphological semantics both at the ROI-level (4×4mm2) and at the WSI-level with the help of visual and language supervisory signals:
1. **Stage 1** vision-only unimodal pretraining with Mass-340K on ROI crops (Figure 1C),
2. **Stage 2** cross-modal alignment of generated morphological descriptions at ROI-level (423K pairs of 8K×8K ROIs and captions),  
3. **Stage 3** cross-modal alignment at WSI-level (183K pairs of WSIs and clinical reports, Figure 1D).

- Pretraining Dataset of 335,645 whole-slide images (WSIs) from diverse cases at Mass General Brigham
- Over 182,000 pathology reports  
- More than 423,000 synthetic captions generated by PathChat
"""

from transformers import AutoModel

# Load TITAN model
titan = AutoModel.from_pretrained('MahmoodLab/TITAN', trust_remote_code=True)

import h5py
import os
import pickle
import yaml
from pathlib import Path
from tqdm import tqdm

import numpy as np
import pandas as pd
import torch
from transformers import AutoModel

from TITAN.titan.utils import get_eval_metrics, TEMPLATES, bootstrap

os.environ["OMP_NUM_THREADS"] = "8"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# load model from huggingface
model = AutoModel.from_pretrained('MahmoodLab/TITAN', trust_remote_code=True)
model = model.to(device)

"""- Extracts features (image embeddings)
- Coords (spatial locations of patches)
- Metadata (patch size)
"""

# load example data
from huggingface_hub import hf_hub_download
demo_h5_path = hf_hub_download(
    "MahmoodLab/TITAN",
    filename="TCGA_demo_features/TCGA-PC-A5DK-01Z-00-DX1.C2D3BC09-411F-46CF-811B-FDBA7C2A295B.h5",
)

import h5py

import torch

import numpy as np

import matplotlib.pyplot as plt

from huggingface_hub import hf_hub_download

from PIL import Image



# Load the dataset

demo_h5_path = hf_hub_download(

    "MahmoodLab/TITAN",

    filename="TCGA_demo_features/TCGA-PC-A5DK-01Z-00-DX1.C2D3BC09-411F-46CF-811B-FDBA7C2A295B.h5",

)



# Open the h5 file

file = h5py.File(demo_h5_path, 'r')



# Extract features and coordinates

features = torch.from_numpy(file['features'][:])  # Assuming these are meaningful image features

coords = torch.from_numpy(file['coords'][:])  # (N, 2) coordinates

patch_size_lv0 = file['coords'].attrs['patch_size_level0']  # Patch size



# Normalize coordinates for visualization

x_coords = coords[:, 0].numpy()

y_coords = coords[:, 1].numpy()



# Convert features to a grayscale intensity (assuming meaningful numeric values)

feature_intensity = features.norm(dim=1).numpy()  # Compute the norm as intensity



# Normalize intensities

feature_intensity = (feature_intensity - feature_intensity.min()) / (feature_intensity.max() - feature_intensity.min())



# Create scatter plot representation

fig, ax = plt.subplots(figsize=(10, 10))

sc = ax.scatter(x_coords, y_coords, cmap='viridis', s=1)  # Small dot size for high resolution

ax.set_title("Feature Map Visualization")

ax.set_xticks([])

ax.set_yticks([])

plt.colorbar(sc, label="Feature Intensity")



# Save the visualization as JPG

#jpg_path = "/mnt/data/feature_map.jpg"

#plt.savefig(jpg_path, dpi=300, bbox_inches='tight', pad_inches=0)

plt.show()



# Return the generated JPG file

#jpg_path



features.shape

coords.shape

patch_size_lv0.shape

"""- **annots**: Aannotations or labels (tissue types, disease status)

- **annots_patching**: Annotations that are specific to patches (smaller sub-regions of the full dataset)

- **coords**: Coordinates or spatial information (image patches, regions of interest in a tissue sample, etc.)

- **coords_patching**: Coordinates within specific patches, detailing the positions of sub-regions.

- **features**: Pixel intensities in an image or other numerical representations that are used for machine learning tasks.

- **mask**: Regions of the data are relevant (e.g., 0 for background and 1 for the region of interest).

- **stitch**: Data about how smaller patches are stitched together to form the full image. It may store stitching parameters or the process to reconstruct the original data.
"""

import h5py

with h5py.File(demo_h5_path, 'r') as file:
    print("File contents:")
    for key in file.keys():
        print(key,file[key].shape)

   # print(" ________Attributes ________")
   # attribute = file['coords']
   # for attr in attribute.attrs:
   #     print(f"{attr}: {attribute.attrs[attr]}")



import numpy
print(numpy.__version__)

file = h5py.File(demo_h5_path, 'r')

stitch = torch.from_numpy(file['stitch'][:])
mask = torch.from_numpy(file['mask'][:])
annots = torch.from_numpy(file['annots'][:])
annots_patching = torch.from_numpy(file['annots_patching'][:])
features = torch.from_numpy(file['features'][:])
coords = torch.from_numpy(file['coords'][:])
coords_patching = torch.from_numpy(file['coords_patching'][:])

patch_size_lv0 = file['coords'].attrs['patch_size_level0']

# pixels high|pixels wide
print("Stitch Shape",stitch.shape)
# stitch Shape|3 channels
print("Mask Shape",mask.shape)
# single data sample|number of patch|class label
print("Annotaion", annots.shape)
#patch|single data sample
print("Annotaion Patching Shape", annots_patching.shape)
#batch|patch|feature
print("Feature Shape",features.shape)
#batch|patch|coordinates
print("Coords Shape",coords.shape)
#patch|coordinates
print("Coords Patching Shape",coords_patching.shape)

"""Converts patch features into a slide-level embedding.( slide_embedding is a dynamic output)"""
print(demo_h5_path)

plt.imshow(stitch)

with torch.autocast('cuda', torch.float16), torch.inference_mode():
    features = features.to(device)
    coords = coords.to(device)
    single_slide_embedding = model.encode_slide_from_patch_features(features, coords, patch_size_lv0)

single_slide_embedding.shape

"""Loads classification prompts and label mappings from a YAML config file."""

with open('./TITAN/datasets/config_tcga-ot.yaml', 'r') as file:
    task_config = yaml.load(file, Loader=yaml.FullLoader)
print("Full YAML Content:")
for key in task_config.keys():
    print(key , task_config[key])

num_samples = task_config['num_samples']
print("Number Samples:", num_samples)
num_classes = task_config['num_classes']
print("Number Classes:",num_classes)
metric = task_config['metric']
print("Metric:", metric)
label_dict = task_config['label_dict']
print("Label Dictionary:", label_dict)
class_prompts = task_config['prompts']
print("Prompts:", class_prompts)
task = task_config['task']
print("Task :",task)

import pandas as pd
import yaml

df = pd.read_csv("./tcga-reports.csv")

yaml_path = "./TITAN/datasets/config_tcga-ot.yaml"
with open(yaml_path, "r") as file:
    task_configg = yaml.safe_load(file)

label_dict = task_configg["label_dict"]

tcga_code_to_label = {
    "BP": "BLCA", "D7": "COAD", "EI": "GBM", "EB": "HCC",
    "A6": "LUAD", "29": "PAAD", "24": "PRAD", "IN": "SKCM",
    "44": "STAD", "KK": "THPA", "CR": "THYM", "C4": "UCEC",
    "HU": "USC", "BR": "UCS", "SX": "UM", "MH": "ACC",
    "DU": "AASTR", "56": "CESC", "34": "ESCA"
}

def extract_tcga_code(filename):
    parts = filename.split("-")
    return parts[1] if len(parts) > 2 else None

df["tcga_code"] = df["patient_filename"].apply(extract_tcga_code)

df["label_code"] = df["tcga_code"].map(tcga_code_to_label)

df_filtered = df.dropna(subset=["label_code"]).copy()

df_filtered["prompts"] = df_filtered["text"]

new_prompts = df_filtered.groupby("label_code")["prompts"].apply(list).to_dict()

task_configg["prompts"] = new_prompts

output_yaml_path = "./config_tcga-ot-updated.yaml"
with open(output_yaml_path, "w") as file:
    yaml.dump(task_configg, file, default_flow_style=False)

print(f"Updated YAML saved at: {output_yaml_path}")

with open('./config_tcga-ot-updated.yaml', 'r') as file:
    task_config = yaml.load(file, Loader=yaml.FullLoader)
print("Full YAML Content:")
for key in task_config.keys():
    print(key)

num_samples = task_config['num_samples']
print("Number Samples:", num_samples)
num_classes = task_config['num_classes']
print("Number Classes:",num_classes)
metric = task_config['metric']
print("Metric:", metric)
label_dict = task_config['label_dict']
print("Label Dictionary:", label_dict)
class_prompts = task_config['prompts']
print("Prompts:", class_prompts)
task = task_config['task']
print("Task :",task)

"""Generates a classifier using TITAN’s zero-shot learning capabilities. Using prompts to classify a sample based on its textual description"""

# create prompts for zero-shot classification
sorted_class_prompts = dict(sorted(class_prompts.items(), key=lambda item: label_dict.get(item[0], float('inf'))))
classes = list(sorted_class_prompts.keys())
class_prompts = [sorted_class_prompts[key] for key in sorted_class_prompts.keys()]
with torch.autocast('cuda', torch.float16), torch.inference_mode():
    classifier = model.zero_shot_classifier(class_prompts, TEMPLATES, device=device)

"""- Predicted class
- Confidence scores for each cancer type
"""

#with torch.autocast('cuda', torch.float16), torch.inference_mode():
#    scores = model.zero_shot(single_slide_embedding, classifier)
#print("Predicted class:", classes[scores.argmax()])
with torch.autocast('cuda', torch.float16), torch.inference_mode():
    scores = model.zero_shot(single_slide_embedding, classifier)
    predicted_class = classes[scores.argmax()]
    confidence = scores.max().item()

print("Predicted class:", predicted_class)
wandb.log({"predicted_class": predicted_class, "confidence": confidence})

print("Normalized similarity scores:", [f"{c}: {score:.3f}" for c, score in zip(classes, scores[0][0])])

train_csv = pd.read_csv('./TITAN/datasets/tcga-ot_train.csv')
slide_id = train_csv['slide_id'][:]
print("slide_id:",len(slide_id))
val_csv = pd.read_csv('./TITAN/datasets/tcga-ot_val.csv')
slide_id = val_csv['slide_id'][:]
print("slide_id:",len(slide_id))
task_csv = pd.read_csv('./TITAN/datasets/tcga-ot_test.csv')
slide_id = task_csv['slide_id'][:]
print("slide_id:",len(slide_id))
#sum = 11186

import pickle
from huggingface_hub import hf_hub_download
slide_feature_path = hf_hub_download(
    "MahmoodLab/TITAN",
    filename="TCGA_TITAN_features.pkl",
)
with open(slide_feature_path, 'rb') as file:
  data = pickle.load(file)
slide_embeddings = torch.from_numpy(data['embeddings'][:])
print("slide_embeddings:",len(slide_embeddings))
slide_names = np.array(data['filenames'])
print("slide_names:",len(slide_names))

"""Filters slides based on a task CSV file containing ground-truth labels."""

slide_names_series = pd.Series(slide_names)
indices = slide_names_series[slide_names_series.isin(task_csv['slide_id'])].index
slide_embeddings = slide_embeddings[indices]
slide_names = slide_names[indices]
print("slide_embeddings:",len(slide_embeddings))
print("slide_names:",len(slide_names))
print("slide_embeddings shape",slide_embeddings.shape)

"""- probs: Probability scores for each class.
- targets: Ground-truth labels for evaluation.
"""

probs = []
targets = []

#for slide_emb, slide_id in tqdm(zip(slide_embeddings, slide_names), total=len(slide_embeddings)):
#    with torch.autocast('cuda', torch.float16), torch.inference_mode():
#        slide_emb = slide_emb.to(device)
#        probs.append(model.zero_shot(slide_emb, classifier).cpu())
for i, (slide_emb, slide_id) in enumerate(tqdm(zip(slide_embeddings, slide_names), total=len(slide_embeddings))):
    with torch.autocast('cuda', torch.float16), torch.inference_mode():
        slide_emb = slide_emb.to(device)
        probs.append(model.zero_shot(slide_emb, classifier).cpu())

    # Log progress every 10 slides
    if i % 10 == 0:
        wandb.log({"processed_slides": i})

        # Use the correct column, e.g., 'OncoTreeCode'
        target_value = task_csv[task_csv['slide_id'] == slide_id]['OncoTreeCode'].values[0]
        targets.append(label_dict[target_value])

probs_all = torch.cat(probs, dim=0)
targets_all = torch.tensor(targets)
preds_all = probs_all.argmax(dim=1)

print("\nprobs_all:", probs_all.shape)
print("targets_all:", targets_all.shape)
print("preds_all:", preds_all.shape)

"""- Deterministic Model: it doesn't use dropout layers or stochastic sampling, it will always give the same results.
- The classifier has fixed weights, it won’t introduce randomness.
- Setting torch.manual_seed() only affects certain operations (e.g., dropout, weight initialization), but the model doesn't use randomness internally, it won’t change anything.
"""

import numpy as np
import torch
import random


def get_model_predictions(model, embedding, num_runs=10, change_seed=True):
    predictions = []
    for i in range(num_runs):
        if change_seed:
            torch.manual_seed(i * 42)

        model.eval()
        with torch.autocast('cuda', torch.float16), torch.inference_mode():
            output = model.zero_shot(embedding.to(device), classifier).cpu()
            probabilities = torch.softmax(output, dim=1).numpy()
            #print("Raw model output before softmax:", output)

            predictions.append(probabilities)

    predictions = np.array(predictions).squeeze()
    uncertainty = np.var(predictions, axis=0)

    identical_predictions = np.all(np.isclose(predictions, predictions[0], atol=1e-4))
    if identical_predictions:
        print("Identical predictions → Changing random seed strategy.")

    return predictions, uncertainty

single_slide_embedding = single_slide_embedding.to(device)

num_runs = 10

predictions, uncertainty = get_model_predictions(model, single_slide_embedding, num_runs=num_runs)

print("Predicted probabilities:\n", predictions.shape)
print("Uncertainty per class:\n", uncertainty.shape)
print("Predicted probabilities:\n", predictions)
print("Uncertainty per class:\n", uncertainty)

slide_embeddings = slide_embeddings.to(device)
#slide_embeddings = slide_embeddings[0].unsqueeze(0)
#with torch.no_grad():
#    slide_embeddings = torch.nn.functional.dropout(single_slide_embedding, p=0.1, training=True)

num_runs = 10

predictions, uncertainty = get_model_predictions(model, slide_embeddings, num_runs=num_runs)

print("Predicted probabilities:\n", predictions.shape)
print("Uncertainty per class:\n", uncertainty.shape)
#print("Predicted probabilities:\n", predictions)
print("Uncertainty per class:\n", uncertainty)

"""- kappa :Measures agreement between the model and the ground truth,
- kapp-nw: gives more weight to important classes.
- Aucroc: Measures how well the model distinguishes between different classes
"""

#results = get_eval_metrics(targets_all, preds_all, probs_all, roc_kwargs={'multi_class': 'ovo', 'average': 'macro'})
#for key, value in results.items():
#    print(f"{key.split('/')[-1]: <12}: {value:.3f}")
# Log evaluation metrics to W&B
results = get_eval_metrics(targets_all, preds_all, probs_all, roc_kwargs={'multi_class': 'ovo', 'average': 'macro'})
for key, value in results.items():
    print(f"{key.split('/')[-1]: <12}: {value:.3f}")
    wandb.log({key.split('/')[-1]: value})  # Log each metric to W&B

"""Bootstrap is a useful method for assessing the uncertainty in model's predictions and statistics.(estimate confidence intervals and quantify the robustness of  results without making strong assumptions)"""

outputs = {
    "targets": targets_all,
    "preds": preds_all,
    "probs": probs_all,
}
bootstrap_kwargs = {'n': 1000, 'alpha': 0.95}
results_mean, results_std = bootstrap(results_dict=outputs, **bootstrap_kwargs)
for keys, values in results_mean.items():
    print(f"{keys.split('/')[-1]: <12}: {values:.4f} ± {results_std[keys]:.4f}")

import numpy as np
import torch


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

specific_embeddings = []

for slide_emb, slide_id in zip(slide_embeddings, slide_names):
    slide_emb = slide_emb.to(device)

    with torch.autocast('cuda', torch.float16), torch.inference_mode():
        scores = model.zero_shot(slide_emb, classifier)

    predicted_class_index = scores.argmax()

    predicted_class = classes[predicted_class_index]

    if predicted_class == 'BLCA':
        specific_embeddings.append(slide_emb.cpu())  # Store the embedding on the CPU to avoid memory issues

print(f"Number of slides predicted: {len(specific_embeddings)}")

specific_embeddings_tensor = torch.stack(specific_embeddings).to(device)

for i, slide_emb in enumerate(specific_embeddings_tensor):
    slide_emb = slide_emb.to(device)

    with torch.autocast('cuda', torch.float16), torch.inference_mode():
        scores = model.zero_shot(slide_emb, classifier)

    predicted_class_index = scores.argmax()

    predicted_class = classes[predicted_class_index]

    print(f"Slide {i + 1}:")
    print(f"Predicted class: {predicted_class}")

    similarity_scores = [f"{c}: {score:.3f}" for c, score in zip(classes, scores[0].cpu().detach().numpy())]
    print("Normalized similarity scores:", ", ".join(similarity_scores))
    print("-" * 50)
similarity_matrix = np.zeros((len(specific_embeddings_tensor), len(classes)))

for i, slide_emb in enumerate(specific_embeddings_tensor):
    slide_emb = slide_emb.to(device)

    with torch.autocast('cuda', torch.float16), torch.inference_mode():
        scores = model.zero_shot(slide_emb, classifier)

    similarity_matrix[i, :] = scores[0].cpu().detach().numpy()


print("Similarity Matrix:",similarity_matrix.shape )
sorted_classes_prompts = sorted(zip(classes, class_prompts), key=lambda x: label_dict.get(x[0], float('inf')))
sorted_classes, sorted_class_prompts = zip(*sorted_classes_prompts)

specific_slide_index = 0
specific_predicted_class_index = scores.argmax()
specific_predicted_class = classes[specific_predicted_class_index]
print(f"Full class description: {sorted_class_prompts[sorted_classes.index(specific_predicted_class)]}")

yaml_path = "/TITAN/datasets/config_tcga-ot-updated.yaml"
with open(yaml_path, "r") as file:
    updated_task_config = yaml.safe_load(file)

updated_prompts = updated_task_config["prompts"]

specific_predicted_class = "BLCA"
if specific_predicted_class in updated_prompts:
    print(f"Prompts for {specific_predicted_class}:")
    for i, prompt in enumerate(updated_prompts[specific_predicted_class], 1):
        print(f"{i}. {prompt}")
else:
    print("No prompts found for")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.imshow(similarity_matrix, aspect='auto', cmap='viridis')
plt.colorbar(label='Similarity Score')
plt.xlabel('Classes')
plt.ylabel('Samples')
plt.title('Similarity Scores between Samples and 42 Classes')
plt.xticks(ticks=np.arange(len(classes)), labels=classes, rotation=90)
plt.yticks(ticks=np.arange(len(specific_embeddings_tensor)), labels=[f'Slide {i+1}' for i in range(len(specific_embeddings_tensor))])
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull
from sklearn.decomposition import PCA


pca = PCA(n_components=2)
reduced_matrix = pca.fit_transform(similarity_matrix)

hull = ConvexHull(reduced_matrix)

plt.figure(figsize=(10, 6))

plt.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], color='r', label='LMS Samples')

for i, sample in enumerate(reduced_matrix):
    plt.text(sample[0], sample[1], f'{i+1}', fontsize=12)

for simplex in hull.simplices:
    plt.plot(reduced_matrix[simplex, 0], reduced_matrix[simplex, 1], 'k-', alpha=0.5)

plt.title('Convex Hull for LMS Samples (PCA Reduced)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

probs = []
uncertainties = []
targets = []

for slide_emb, slide_id in tqdm(zip(slide_embeddings, slide_names), total=len(slide_embeddings)):
    preds, uncertainty = get_model_predictions(model, slide_emb, num_runs=5)

    probs.append(torch.tensor(preds).mean(dim=0))
    uncertainties.append(torch.tensor(uncertainty))

    target_value = task_csv[task_csv['slide_id'] == slide_id]['OncoTreeCode'].values[0]
    targets.append(label_dict[target_value])

probs_all = torch.stack(probs)
uncertainties_all = torch.stack(uncertainties)
targets_all = torch.tensor(targets)
preds_all = probs_all.argmax(dim=1)

# Print shapes
print("\nPredictions Shape:", preds_all.shape)
print("Probabilities Shape:", probs_all.shape)
print("Uncertainties Shape:", uncertainties_all.shape)
print("Targets Shape:", targets_all.shape)

torch.save(model.state_dict(), "titan_model.pth")
wandb.save("titan_model.pth")  # Save to W&B
